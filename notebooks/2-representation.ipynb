{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation Notebook\n",
    "In this notebook, I handle feature extraction and vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized dataset\n",
    "tokenized_datasets = load_from_disk(\"./tokenized_imdb\")\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract embeddings\n",
    "def extract_embeddings(batch):\n",
    "    inputs = {k: torch.tensor(v) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return {\"embeddings\": outputs.last_hidden_state[:, 0, :].numpy()}\n",
    "\n",
    "# Apply embedding extraction\n",
    "tokenized_datasets = tokenized_datasets.map(extract_embeddings, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "tokenized_datasets.save_to_disk(\"./embeddings_imdb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
